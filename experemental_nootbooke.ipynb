{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#from RM_tools import dp_utils\n",
    "from pathlib import Path\n",
    "import pickle \n",
    "import datetime\n",
    "from calendar import month_abbr, different_locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% load dicts\n",
    "features_group = pd.read_excel(r'.\\codes\\feature_groups.xlsx')\n",
    "demo_order = pd.read_excel(r'.\\codes\\demo_order.xlsx')\n",
    "demo_order['buyers_gr_label'] = demo_order['buyers_gr_label'].astype(str).str.lower()\n",
    "segments_order = pd.read_excel(r'.\\codes\\segments_order.xlsx')\n",
    "example = pd.read_excel(r'.\\codes\\Example.xlsx', sheet_name = 'example')\n",
    "legends = pd.read_excel(r'.\\codes\\Example.xlsx', sheet_name = 'legends')\n",
    "shop_order = pd.read_excel(r'.\\codes\\shop_order.xlsx')\n",
    "today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "list_of_category = ['Hair Conditioners',\n",
    " 'Laundry Detergents',\n",
    " 'MALE B&R',\n",
    " 'Shampoos',\n",
    " 'Feminine Care',\n",
    " 'Diapers']\n",
    "\n",
    "#get_low!\n",
    "list_of_dict = [features_group,demo_order,\n",
    "                 segments_order,example,legends,shop_order]\n",
    "for d in list_of_dict:\n",
    "    d.columns = [c.lower() for c in d.columns ]\n",
    "\n",
    "fin_cols = example.columns.to_list() + ['shop_code', 'shop_lvls','channel_code', 'file']\n",
    "columns_dict = legends[['mask','Colum'.lower()]].dropna(subset='mask').set_index('mask').to_dict()['Colum'.lower()]\n",
    "\n",
    "\n",
    "\n",
    "def get_category(name_of_file, list_of_category=False):\n",
    "    cat_dict = {'br': 'MALE B&R',\n",
    " 'deterg': 'Laundry Detergents',\n",
    " 'diapers': 'Diapers',\n",
    " 'femcare': 'Feminine Care',\n",
    " 'haircond': 'Hair Conditioners',\n",
    " 'shampoo': 'Shampoos',\n",
    " 'MALE B&R': 'MALE B&R',\n",
    " 'Laundry Detergents': 'Laundry Detergents',\n",
    " 'Diapers': 'Diapers',\n",
    " 'Feminine Care': 'Feminine Care',\n",
    " 'Hair Conditioners': 'Hair Conditioners',\n",
    " 'Shampoos': 'Shampoos'}\n",
    "    for k in cat_dict.keys():\n",
    "        if name_of_file.count(k):\n",
    "            return cat_dict[k]\n",
    "    else:\n",
    "        raise ValueError('В названии файла нет категории', name_of_file)    \n",
    "\n",
    "\n",
    "# %% make_functions\n",
    "\n",
    "def get_period_lbl(time_period_type, year, month):\n",
    "    with different_locale('English'):\n",
    "        return f\"{time_period_type.split('/')[0]}: {month_abbr[month]} {year}\"\n",
    "\n",
    "def get_label_num(time_period_type, year, month):\n",
    "    with different_locale('English'):\n",
    "        return f\"{time_period_type.split('/')[0]}: {year} ({str(month).zfill(2)}) {month_abbr[month]}\"\n",
    "\n",
    "\n",
    "def add_period_lbls(df_in):\n",
    "    periods = pd.read_excel(r'.\\codes\\periods.xlsx', sheet_name ='period_lbl').drop(columns='Unnamed: 0')\n",
    "    time_period_type = pd.read_excel(r'.\\codes\\periods.xlsx', sheet_name ='time_period_type')\n",
    "\n",
    "\n",
    "    time_period_type_dict = {24 :'2MATs/ 104 we', 12:'MAT/ 52 we', 3:'3MMT/ 12we', 1: 'Month'}\n",
    "    \n",
    "    col_to_int = ['duration', 'year', 'month']\n",
    "    df_in[col_to_int] = df_in[col_to_int].astype(int)\n",
    "    \n",
    "    df_in['time_period_type'] = df_in['duration'].map(time_period_type_dict)\n",
    "    \n",
    "    df_tmp_per = df_in[['time_period_type', 'year', 'month']].drop_duplicates()\n",
    "    df_tmp_per['period_lbl'] = df_tmp_per[['time_period_type', 'year', 'month']]\\\n",
    "        .apply(lambda row: get_period_lbl(*row), axis=1)\n",
    "    \n",
    "    df_tmp_per['label_num'] = df_tmp_per[['time_period_type', 'year', 'month']]\\\n",
    "        .apply(lambda row: get_label_num(*row), axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    new_periods = set(df_tmp_per['period_lbl']) - set(periods['period_lbl'])\n",
    "    if len(new_periods):\n",
    "        print('New periods!')\n",
    "        print(*new_periods)\n",
    "        new_periods_df = df_tmp_per[df_tmp_per['period_lbl'].isin(new_periods)].sort_values(by=['year','month'])\n",
    "        new_periods_df['date_added'] = today\n",
    "        periods = pd.concat([periods[['period_lbl','label_num','date_added' ]], new_periods_df[['period_lbl','label_num','date_added' ]]],ignore_index=True )\n",
    "        periods['period_code'] = range(1,len(periods)+1)\n",
    "\n",
    "\n",
    "        with pd.ExcelWriter(r'.\\codes\\periods.xlsx',engine=\"openpyxl\",mode=\"a\", if_sheet_exists='replace') as writer:\n",
    "            periods.to_excel(writer, sheet_name ='period_lbl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_in = df_in.merge(df_tmp_per,on = ['time_period_type', 'year', 'month'], how='left')\n",
    "    \n",
    "    df_in = df_in.merge(\n",
    "            periods, on=['period_lbl', 'label_num'],  how='left', validate='many_to_one')\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_in = df_in.merge(\n",
    "           time_period_type, on='time_period_type', how='left', validate='many_to_one')\n",
    "\n",
    "    df_in = df_in.drop(columns=['time_period_type', 'period_lbl', 'month' ])\\\n",
    "            .rename(columns={'time_period_code': 'time_period_type', 'period_code': 'period_lbl'})\n",
    "\n",
    "    return df_in\n",
    "\n",
    "def get_df_in_v2(df_in, category, columns_dict):\n",
    "    \n",
    "    #get_low_2!\n",
    "    df_in.columns = [c.lower() for c in df_in.columns]\n",
    "    income_columns = list(df_in.columns)\n",
    "\n",
    "\n",
    "    df_in = df_in.rename(\n",
    "        {'Category Name'.lower():'Product Name'.lower(), 'Buyer Group Name'.lower():'buyers_gr_label'}, \n",
    "        axis=1)\n",
    "    df_in['Category Name'.lower()] = category\n",
    "    df_in['buyers_gr_label'] = df_in['buyers_gr_label'].replace({'Total Population' :'TOTAL DEMOGRAPHICS'})\n",
    "    df_in = df_in.drop('category',axis=1)\n",
    "\n",
    "\n",
    "    #print('init shape', df_in.shape)\n",
    "\n",
    "\n",
    "        \n",
    "    # products\n",
    "    #prod_col_to_merge = 'Product Name'.lower()\n",
    "    #if  len(set(df_in['Product Name'.lower()].unique()) - set(features_group['Product Name'.lower()]))>0:\n",
    "    #    print('Мерджим по фулл нейму')\n",
    "    #    prod_col_to_merge = 'full_label'\n",
    "    #    features_group = features_group.drop(columns='Product Name'.lower())\n",
    "    prod_col_to_merge = 'full_label'\n",
    "\n",
    "    \n",
    "    df_in = df_in.merge(\n",
    "        features_group, \n",
    "        left_on=['Category Name'.lower(), 'Product Name'.lower()], \n",
    "        right_on=['Category Name'.lower(),  prod_col_to_merge], \n",
    "        how='left', validate='many_to_one') #Есть множественные ключи в features_group['Product Name']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_in = df_in.rename(\n",
    "        columns={'product_code': 'products', 'category_code': 'category', 'feature_code': 'features'}\n",
    "    )\n",
    "    df_in['product_hier'] = df_in['products']\n",
    "\n",
    "\n",
    "    # socdem\n",
    "    df_in['buyers_gr_label'] = df_in['buyers_gr_label'].str.lower()\n",
    "    df_in = df_in.merge(\n",
    "        demo_order, on=['buyers_gr_label'], how='left', validate='many_to_one')\n",
    "    df_in = df_in.rename(columns={'demo_code': 'demo_groups'})\n",
    "    df_in['demo_hier'] = df_in['demo_groups']\n",
    "\n",
    "    \n",
    "    # segments\n",
    "\n",
    "    if ('Segment'.lower() not in df_in.columns):\n",
    "        if (category == 'Feminine Care'):\n",
    "            df_in['Segment'.lower()] = 'Total Feminine Care'\n",
    "        elif (category == 'Shampoos'):\n",
    "            df_in['Segment'.lower()] = 'Shampoos'\n",
    "    \n",
    "\n",
    "    \n",
    "    if (category == 'Laundry Detergents'):\n",
    "        df_in['Segment'.lower()] = df_in['Segment'.lower()].replace(\n",
    "            {'Total Detergents excluding Bars': 'Total Detergents excluding Bar'})\n",
    "    df_in['Segment'.lower()] =df_in['Segment'.lower()].replace({'Total Diapers size':'Total Diapers'})\n",
    "    df_in = df_in.merge(\n",
    "        segments_order, on=['Segment'.lower()], how='left', validate='many_to_one')\n",
    "    df_in = df_in.rename(columns={'segment_code': 'cat_segment'})\n",
    "\n",
    "\n",
    "    # shop\n",
    "    df_in['position_name_shop'] = df_in['position_name_shop'].replace(\n",
    "    {'Ok Hyper': 'OK Hyper', 'Perekryostok': 'Perekrestok'})\n",
    "    df_in = df_in.merge(\n",
    "        shop_order, on=['position_name_shop'], how='left', validate='many_to_one')\n",
    "\n",
    "    \n",
    "    #print('merged shape', df_in.shape)\n",
    "    \n",
    "    # rename metrics\n",
    "    df_in.columns = [col.lower().replace(' ','_') for col in df_in.columns]\n",
    "    df_in = df_in.rename(columns_dict, axis=1)\n",
    "    df_in = df_in[[i for i in df_in.columns if i in fin_cols + ['duration','month'] + income_columns ]]\n",
    "    \n",
    "    return df_in\n",
    "    \n",
    "def get_buyers_shares(df_in): # %% value / buyers shares for socdem\n",
    "    cols = ['product_lvls', 'category',\n",
    "        'shop_code', 'shop_lvls',\n",
    "        'time_period_type', 'year', 'period_lbl',\n",
    "        'product_hier', 'products', \n",
    "        'features', 'cat_segment'] \n",
    "    cols_to_rename = {'spend_local_currency': 'total_spend_local_currency', \n",
    "                'buying_households': 'total_buying_households'}\n",
    "    cols_to_merge = list(df_in.columns[df_in.dtypes != 'float64'].difference([*demo_order.columns, 'demo_groups']))\n",
    "\n",
    "        \n",
    "    totals = df_in.loc[df_in['demo_groups'] == 1,[*cols_to_merge,'spend_local_currency','buying_households']].rename(columns=cols_to_rename)\n",
    "    totals = totals.drop_duplicates()\n",
    "    df_in = df_in.merge(totals, how='left',  validate='many_to_one')\n",
    "\n",
    "    df_in['value_share'] = df_in['spend_local_currency'] / df_in['total_spend_local_currency'] * 100\n",
    "    df_in['buyers_share'] = df_in['buying_households'] / df_in['total_buying_households'] * 100\n",
    "\n",
    "\n",
    "    df_in = df_in.drop(columns=['total_spend_local_currency', 'total_buying_households'])\n",
    "    \n",
    "    return df_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diapers\n",
      "(1578114, 50)\n",
      "Feminine Care\n",
      "(2554552, 50)\n",
      "Hair Conditioners\n",
      "(3561345, 50)\n",
      "Laundry Detergents\n",
      "(7939944, 50)\n",
      "MALE B&R\n",
      "(2327033, 50)\n",
      "Shampoos\n",
      "(1372281, 50)\n",
      "Creating united data file complite! (19333269, 50)\n",
      "Колонок без словаря {'duration', 'segment', 'file', 'position_name_shop'}\n",
      "label_num\n",
      "Колонок с пропусками кодов 1\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#from RM_tools import dp_utils\n",
    "from pathlib import Path\n",
    "import pickle \n",
    "chunk_size = 1_000_000\n",
    "\n",
    "\n",
    "# %% load old format data\n",
    "\n",
    "files = Path('./data/tmp').glob('*.pq')\n",
    "dfs = []\n",
    "for file in files:\n",
    "    print(file.stem)\n",
    "    tmp = pd.read_parquet(file)\n",
    "    print(tmp.shape)\n",
    "    \n",
    "    dfs.append(tmp)\n",
    "\n",
    "\n",
    "df_union = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "del dfs\n",
    "\n",
    "print(\"Creating united data file complite!\",df_union.shape )\n",
    "\n",
    "\n",
    "# %% value / buyers shares for socdem\n",
    "# cols = ['product_lvls', 'category',\n",
    "#         'shop_code', 'shop_lvls',\n",
    "#         'time_period_type', 'year', 'period_lbl',\n",
    "#         'product_hier', 'products', \n",
    "#         'features', 'cat_segment','demo_hier'] \n",
    "# cols_to_rename = {'spend_local_currency': 'total_spend_local_currency', \n",
    "#              'buying_households': 'total_buying_households'}\n",
    "        \n",
    "# totals = df_union.loc[df_union['demo_groups'] == 1, cols+ [*cols_to_rename.keys()]].rename(\n",
    "#     columns={'spend_local_currency': 'total_spend_local_currency', \n",
    "#              'buying_households': 'total_buying_households'})\n",
    "# totals = totals.drop_duplicates()\n",
    "\n",
    "# df_union = df_union.merge(totals, how='left',left_on =cols + [*cols_to_rename.keys()], right_on =cols +[*cols_to_rename.values()],  validate='many_to_one')\n",
    "\n",
    "# df_union['value_share'] = df_union['spend_local_currency'] / df_union['total_spend_local_currency'] * 100\n",
    "# df_union['buyers_share'] = df_union['buying_households'] / df_union['total_buying_households'] * 100\n",
    "\n",
    "\n",
    "\n",
    "#df_union = df_union.drop(columns=['total_spend_local_currency', 'total_buying_households'])\n",
    "#del totals\n",
    "\n",
    "\n",
    "# %% variable labels\n",
    "metrics = {\n",
    "    'Buying Households': 'Buyers (000 HH)',\n",
    "    # 'Projected Shoppers': 'Buyers (000 HH)',\n",
    "    # 'Panel Sample Size': 'Population Raw',\n",
    "    'Raw Shoppers': 'Raw Buyers',\n",
    "    'Loyalty Volume Based': 'Loyalty Volume',\n",
    "    'Loyalty Value Based Local Currency': 'Loyalty Value RUB',\n",
    "    # 'Loyalty Value Based EUR': 'Loyalty Value EUR',\n",
    "    'Loyalty Value Based USD': 'Loyalty Value USD',    \n",
    "    'Percent 2+ Time Buyers': 'Repeat Rate (percent of buyers with frequency 2 and more)',\n",
    "    'Percent Household Penetration': 'Penetration',\n",
    "    'Purchase Frequency': 'Frequency',\n",
    "    'Occasions': 'Trips (000)',\n",
    "    'Item Buying Rate local currency': 'Spend per buyer RUB',\n",
    "    # 'Item Buying Rate EUR': 'Spend per buyer EUR',\n",
    "    'Item Buying Rate USD': 'Spend per buyer USD',   \n",
    "    'Purchase size local currency': 'Spend per trip RUB',\n",
    "    # 'Purchase size EUR': 'Spend per trip EUR',\n",
    "    'Purchase size USD': 'Spend per trip USD',    \n",
    "    'Purchase size SU': 'Volume per trip SU',\n",
    "    'Average per SU local currency': 'Average Price per SU (RUB)',\n",
    "    # 'Average per SU EUR': 'Average Price L/kg/SU/EUR',\n",
    "    'Average per SU USD': 'Average Price per SU USD',\n",
    "    'Volume SU': 'Volume SU (000)',\n",
    "    'Volume Physical Units': 'Volume Packs (000)',\n",
    "    'Spend Local Currency': 'Value RUB (000)',\n",
    "    # 'Spend EUR': 'Value EUR',\n",
    "    'Spend USD': 'Value USD (000)',    \n",
    "}\n",
    "\n",
    "metrics_dic = {\n",
    "    k.lower().replace(' ', '_').replace('+', '_'): v for k, v in metrics.items()\n",
    "}\n",
    "\n",
    "metrics_dic.update({\n",
    "    'value_share': 'Value share, % of Total Demography',\n",
    "    'buyers_share': 'Buyers share, % of Total Demography'\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "#make lbl_dict\n",
    "product_order = pd.read_excel('codes/feature_groups.xlsx')\n",
    "demo_order = pd.read_excel('codes/demo_order.xlsx')\n",
    "segment_order = pd.read_excel('codes/segments_order.xlsx')\n",
    "shop_order = pd.read_excel('codes/shop_order.xlsx')\n",
    "period_dic = pd.read_excel('codes/periods.xlsx', sheet_name=None)\n",
    "\n",
    "\n",
    "lbl_dic = {}\n",
    "cat_new_codes = {\n",
    "    '0. Diapers': 1, \n",
    "    '0. MALE B&R': 2, \n",
    "    '0. Conditioners': 3,\n",
    "    '0. Shampoos': 4,\n",
    "    '0. Total Feminine Care': 5, \n",
    "    '0. Laundry Detergents': 6\n",
    "}\n",
    "lbl_dic['category'] = dict(zip(product_order['category_code'], product_order['Category Name']))\n",
    "lbl_dic['level_0'] = lbl_dic['category']\n",
    "\n",
    "lbl_dic['features'] = dict(zip(product_order['feature_code'], product_order['feature']))\n",
    "\n",
    "lbl_dic['cat_segment'] = dict(zip(segment_order['segment_code'], segment_order['Segment']))\n",
    "\n",
    "#product_new_codes = dict(zip(product_order['Vendor Product ID'], product_order['product_code']))\n",
    "lbl_dic['products'] = dict(zip(product_order['product_code'], product_order['full_label']))\n",
    "\n",
    "lbl_dic['product_hier'] = dict(zip(product_order['product_code'], product_order['product_hier']))\n",
    "\n",
    "demo_new_codes = dict(zip(demo_order['Buyer Group ID'], demo_order['demo_code']))\n",
    "lbl_dic['demo_groups'] = dict(zip(demo_order['demo_code'], demo_order['buyers_gr_label']))\n",
    "\n",
    "\n",
    "lbl_dic['shop_code'] = dict(zip(shop_order['shop_code'], shop_order['position_name_shop']))\n",
    "lbl_dic['shop_lvls'] = dict(zip(shop_order['shop_lvls'], shop_order['shop_lvls'].astype(str)))\n",
    "lbl_dic['channel_code'] = dict(zip(shop_order['channel_code'], shop_order['channel_type']))\n",
    "\n",
    "lbl_dic['product_lvls'] = {k: str(k) for k in product_order['product_lvls']}\n",
    "lbl_dic['demo_lvls'] = {k: str(k) for k in demo_order['demo_lvls']}\n",
    "\n",
    "lbl_dic['year'] = dict(zip(period_dic['year']['year_code'], period_dic['year']['year']))\n",
    "lbl_dic['time_period_type'] = dict(zip(\n",
    "    period_dic['time_period_type']['time_period_code'], \n",
    "    period_dic['time_period_type']['time_period_type']))\n",
    "\n",
    "lbl_dic['label_num'] = dict(zip(period_dic['period_lbl']['period_code'], period_dic['period_lbl']['label_num']))\n",
    "\n",
    "lbl_dic['period_lbl'] = dict(zip(period_dic['period_lbl']['period_code'], period_dic['period_lbl']['period_lbl']))\n",
    "\n",
    "lbl_dic['socdem_gr'] = dict(zip(demo_order['demo_code'] , demo_order['buyers_gr_label'] ))\n",
    "lbl_dic['demo_groups'] = dict(zip(demo_order['demo_code'] , demo_order['buyers_gr_label'] ))\n",
    "lbl_dic['demo_hier'] = dict(zip(demo_order['demo_code'] , demo_order['demo_hier'] ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Колонок без словаря\",set(df_union.select_dtypes(exclude='float').columns) - set(metrics_dic.keys()|lbl_dic.keys() ))\n",
    "\n",
    "values_no_code ={} \n",
    "for col  in [key for key in lbl_dic if key in df_union.columns]:\n",
    "    col_codes = set(lbl_dic[col].keys())\n",
    "    col_values = set(df_union[col].unique())\n",
    "    non_code_values_col =  col_values - col_codes\n",
    "    if len(non_code_values_col):\n",
    "        values_no_code[col] = non_code_values_col\n",
    "        print(col)\n",
    "\n",
    "if len(values_no_code):\n",
    "    print(\"Колонок с пропусками кодов\", len(values_no_code))\n",
    "    with open('data/tmp/bad_codes.pickle', 'wb') as f:\n",
    "        pickle.dump(values_no_code, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_num\n"
     ]
    }
   ],
   "source": [
    "values_no_code ={} \n",
    "for col  in [key for key in lbl_dic if key in df_union.columns]:\n",
    "    col_codes = set(lbl_dic[col].keys())\n",
    "    col_values = set(df_union[col].unique())\n",
    "    non_code_values_col =  col_values - col_codes\n",
    "    if len(non_code_values_col):\n",
    "        values_no_code[col] = non_code_values_col\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "col = 'demo_lvls'\n",
    "col_codes = set(lbl_dic[col].keys())\n",
    "col_values = set(df_union[col].unique())\n",
    "non_code_values_col =  col_values - col_codes\n",
    "print(non_code_values_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(8),\n",
       " np.int64(13),\n",
       " np.int64(18),\n",
       " np.int64(22),\n",
       " np.int64(27),\n",
       " np.int64(38),\n",
       " np.int64(45),\n",
       " np.int64(50),\n",
       " np.int64(54),\n",
       " np.int64(58)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_union['socdem_gr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(lbl_dic['socdem_gr'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tmp/bad_codes.pickle', 'rb') as f:\n",
    "    values_no_code = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_num': {'2MATs: 2021 (10) Oct',\n",
       "  '2MATs: 2021 (11) Nov',\n",
       "  '2MATs: 2021 (12) Dec',\n",
       "  '2MATs: 2022 (01) Jan',\n",
       "  '2MATs: 2022 (02) Feb',\n",
       "  '2MATs: 2022 (03) Mar',\n",
       "  '2MATs: 2022 (04) Apr',\n",
       "  '2MATs: 2022 (05) May',\n",
       "  '2MATs: 2022 (06) Jun',\n",
       "  '2MATs: 2022 (07) Jul',\n",
       "  '2MATs: 2022 (08) Aug',\n",
       "  '2MATs: 2022 (09) Sep',\n",
       "  '2MATs: 2022 (10) Oct',\n",
       "  '2MATs: 2022 (11) Nov',\n",
       "  '2MATs: 2022 (12) Dec',\n",
       "  '2MATs: 2023 (01) Jan',\n",
       "  '2MATs: 2023 (02) Feb',\n",
       "  '2MATs: 2023 (03) Mar',\n",
       "  '2MATs: 2023 (04) Apr',\n",
       "  '2MATs: 2023 (05) May',\n",
       "  '2MATs: 2023 (06) Jun',\n",
       "  '2MATs: 2023 (07) Jul',\n",
       "  '2MATs: 2023 (08) Aug',\n",
       "  '2MATs: 2023 (09) Sep',\n",
       "  '2MATs: 2023 (10) Oct',\n",
       "  '2MATs: 2023 (11) Nov',\n",
       "  '2MATs: 2023 (12) Dec',\n",
       "  '2MATs: 2024 (01) Jan',\n",
       "  '2MATs: 2024 (02) Feb',\n",
       "  '2MATs: 2024 (03) Mar',\n",
       "  '2MATs: 2024 (04) Apr',\n",
       "  '2MATs: 2024 (05) May',\n",
       "  '2MATs: 2024 (06) Jun',\n",
       "  '2MATs: 2024 (07) Jul',\n",
       "  '2MATs: 2024 (08) Aug',\n",
       "  '2MATs: 2024 (09) Sep',\n",
       "  '3MMT: 2021 (10) Oct',\n",
       "  '3MMT: 2021 (11) Nov',\n",
       "  '3MMT: 2021 (12) Dec',\n",
       "  '3MMT: 2022 (01) Jan',\n",
       "  '3MMT: 2022 (02) Feb',\n",
       "  '3MMT: 2022 (03) Mar',\n",
       "  '3MMT: 2022 (04) Apr',\n",
       "  '3MMT: 2022 (05) May',\n",
       "  '3MMT: 2022 (06) Jun',\n",
       "  '3MMT: 2022 (07) Jul',\n",
       "  '3MMT: 2022 (08) Aug',\n",
       "  '3MMT: 2022 (09) Sep',\n",
       "  '3MMT: 2022 (10) Oct',\n",
       "  '3MMT: 2022 (11) Nov',\n",
       "  '3MMT: 2022 (12) Dec',\n",
       "  '3MMT: 2023 (01) Jan',\n",
       "  '3MMT: 2023 (02) Feb',\n",
       "  '3MMT: 2023 (03) Mar',\n",
       "  '3MMT: 2023 (04) Apr',\n",
       "  '3MMT: 2023 (05) May',\n",
       "  '3MMT: 2023 (06) Jun',\n",
       "  '3MMT: 2023 (07) Jul',\n",
       "  '3MMT: 2023 (08) Aug',\n",
       "  '3MMT: 2023 (09) Sep',\n",
       "  '3MMT: 2023 (10) Oct',\n",
       "  '3MMT: 2023 (11) Nov',\n",
       "  '3MMT: 2023 (12) Dec',\n",
       "  '3MMT: 2024 (01) Jan',\n",
       "  '3MMT: 2024 (02) Feb',\n",
       "  '3MMT: 2024 (03) Mar',\n",
       "  '3MMT: 2024 (04) Apr',\n",
       "  '3MMT: 2024 (05) May',\n",
       "  '3MMT: 2024 (06) Jun',\n",
       "  '3MMT: 2024 (07) Jul',\n",
       "  '3MMT: 2024 (08) Aug',\n",
       "  '3MMT: 2024 (09) Sep',\n",
       "  'MAT: 2021 (10) Oct',\n",
       "  'MAT: 2021 (11) Nov',\n",
       "  'MAT: 2021 (12) Dec',\n",
       "  'MAT: 2022 (01) Jan',\n",
       "  'MAT: 2022 (02) Feb',\n",
       "  'MAT: 2022 (03) Mar',\n",
       "  'MAT: 2022 (04) Apr',\n",
       "  'MAT: 2022 (05) May',\n",
       "  'MAT: 2022 (06) Jun',\n",
       "  'MAT: 2022 (07) Jul',\n",
       "  'MAT: 2022 (08) Aug',\n",
       "  'MAT: 2022 (09) Sep',\n",
       "  'MAT: 2022 (10) Oct',\n",
       "  'MAT: 2022 (11) Nov',\n",
       "  'MAT: 2022 (12) Dec',\n",
       "  'MAT: 2023 (01) Jan',\n",
       "  'MAT: 2023 (02) Feb',\n",
       "  'MAT: 2023 (03) Mar',\n",
       "  'MAT: 2023 (04) Apr',\n",
       "  'MAT: 2023 (05) May',\n",
       "  'MAT: 2023 (06) Jun',\n",
       "  'MAT: 2023 (07) Jul',\n",
       "  'MAT: 2023 (08) Aug',\n",
       "  'MAT: 2023 (09) Sep',\n",
       "  'MAT: 2023 (10) Oct',\n",
       "  'MAT: 2023 (11) Nov',\n",
       "  'MAT: 2023 (12) Dec',\n",
       "  'MAT: 2024 (01) Jan',\n",
       "  'MAT: 2024 (02) Feb',\n",
       "  'MAT: 2024 (03) Mar',\n",
       "  'MAT: 2024 (04) Apr',\n",
       "  'MAT: 2024 (05) May',\n",
       "  'MAT: 2024 (06) Jun',\n",
       "  'MAT: 2024 (07) Jul',\n",
       "  'MAT: 2024 (08) Aug',\n",
       "  'MAT: 2024 (09) Sep',\n",
       "  'Month: 2021 (10) Oct',\n",
       "  'Month: 2021 (11) Nov',\n",
       "  'Month: 2021 (12) Dec',\n",
       "  'Month: 2022 (01) Jan',\n",
       "  'Month: 2022 (02) Feb',\n",
       "  'Month: 2022 (03) Mar',\n",
       "  'Month: 2022 (04) Apr',\n",
       "  'Month: 2022 (05) May',\n",
       "  'Month: 2022 (06) Jun',\n",
       "  'Month: 2022 (07) Jul',\n",
       "  'Month: 2022 (08) Aug',\n",
       "  'Month: 2022 (09) Sep',\n",
       "  'Month: 2022 (10) Oct',\n",
       "  'Month: 2022 (11) Nov',\n",
       "  'Month: 2022 (12) Dec',\n",
       "  'Month: 2023 (01) Jan',\n",
       "  'Month: 2023 (02) Feb',\n",
       "  'Month: 2023 (03) Mar',\n",
       "  'Month: 2023 (04) Apr',\n",
       "  'Month: 2023 (05) May',\n",
       "  'Month: 2023 (06) Jun',\n",
       "  'Month: 2023 (07) Jul',\n",
       "  'Month: 2023 (08) Aug',\n",
       "  'Month: 2023 (09) Sep',\n",
       "  'Month: 2023 (10) Oct',\n",
       "  'Month: 2023 (11) Nov',\n",
       "  'Month: 2023 (12) Dec',\n",
       "  'Month: 2024 (01) Jan',\n",
       "  'Month: 2024 (02) Feb',\n",
       "  'Month: 2024 (03) Mar',\n",
       "  'Month: 2024 (04) Apr',\n",
       "  'Month: 2024 (05) May',\n",
       "  'Month: 2024 (06) Jun',\n",
       "  'Month: 2024 (07) Jul',\n",
       "  'Month: 2024 (08) Aug',\n",
       "  'Month: 2024 (09) Sep'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_no_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
